geom_boxplot(width = .2, alpha = .6, show.legend = FALSE) +
stat_summary(fun.data = "mean_se", geom = "pointrange", show.legend = F,
position = position_dodge(.175)) +
scale_x_discrete(name = "Condition", labels = c("Non-word", "Word")) +
scale_y_continuous(name = "Reaction time (ms)",
breaks = seq(200, 800, 100),
limits = c(200, 800),
expand = c(0, 0)) +
scale_fill_viridis_d(option = "E", name = "Language group") +
theme_hgrid_config
# Raincloud plots
rain_height <- .1
ggplot(dat_long, aes(x = "", y = rt, fill = language)) +
# clouds
introdataviz::geom_flat_violin(trim=FALSE, alpha = 0.4,
position = position_nudge(x = rain_height+.05)) +
# rain
geom_point(aes(colour = language), size = 2, alpha = .5, show.legend = FALSE,
position = position_jitter(width = rain_height, height = 0)) +
# boxplots
geom_boxplot(width = rain_height, alpha = 0.4, show.legend = FALSE,
outlier.shape = NA,
position = position_nudge(x = -rain_height*2)) +
# mean and SE point in the cloud
stat_summary(fun.data = mean_se, mapping = aes(color = language), show.legend = FALSE,
position = position_nudge(x = rain_height * 3)) +
# adjust layout
scale_x_discrete(name = "", expand = c(rain_height*3, 0, 0, 0.7)) +
scale_y_continuous(name = "Reaction time (ms)",
breaks = seq(200, 800, 100),
limits = c(200, 800)) +
coord_flip() +
facet_wrap(~factor(condition,
levels = c("word", "nonword"),
labels = c("Word", "Non-Word")),
nrow = 2) +
# custom colours and theme
scale_fill_viridis_d(option = "E", name = "Language group") +
scale_colour_viridis_d(option  ="E") +
theme_minimal() +
theme(panel.grid.major.y = element_blank(),
legend.position = c(0.8, 0.8),
legend.background = element_rect(fill = "white", color = "white"),
panel.grid = element_line(linetype = "dashed"))
# Ridge plots
# read in data from Nation et al. 2017
data <- read_csv("https://raw.githubusercontent.com/zonination/perceptions/master/probly.csv")
# convert to long format and percents
long <- pivot_longer(data, cols = everything(),
names_to = "label",
values_to = "prob") %>%
mutate(label = factor(label, names(data), names(data)),
prob = prob/100)
# ridge plot
ggplot(long, aes(x = prob, y = label, fill = label)) +
ggridges::geom_density_ridges(scale = 2, show.legend = FALSE) +
scale_x_continuous(name = "Assigned Probability",
limits = c(0, 1.1), labels = scales::percent,
expand = c(0, 0)
) +
# control space at top and bottom of plot
scale_y_discrete(name = "", expand = c(0.02, 0, .08, 0)) +
theme_dviz_vgrid() +
theme(
panel.grid = element_line(size = .3, linetype = "dashed"),
panel.border = element_blank(),
axis.ticks.y = element_blank()
)
# Alluvial plots
# simulate data for 4 years of grades from 500 students
# with a correlation of 0.75 from year to year
# and a slight increase each year
dat <- faux::sim_design(
within = list(year = c("Y1", "Y2", "Y3", "Y4")),
n = 500,
mu = c(Y1 = 0, Y2 = .2, Y3 = .4, Y4 = .6), r = 0.75,
dv = "grade", long = TRUE, plot = FALSE) %>%
# convert numeric grades to letters with a defined probability
mutate(grade = faux::norm2likert(grade, prob = c("3rd" = 5, "2.2" = 10, "2.1" = 40, "1st" = 20)),
grade = factor(grade, c("1st", "2.1", "2.2", "3rd"))) %>%
# reformat data and count each combination
tidyr::pivot_wider(names_from = year, values_from = grade) %>%
dplyr::count(Y1, Y2, Y3, Y4)
# plot data with colours by Year1 grades
ggplot(dat, aes(y = n, axis1 = Y1, axis2 = Y2, axis3 = Y3, axis4 = Y4)) +
geom_alluvium(aes(fill = Y4), width = 1/6) +
geom_stratum(fill = "grey", width = 1/3, color = "black") +
geom_label(stat = "stratum", aes(label = after_stat(stratum))) +
scale_fill_viridis_d(name = "Final Classification") +
theme_minimal() +
theme(legend.position = "top")
mpg_sum <- mpg |>
dplyr::filter(year == 2008) |>
dplyr::mutate(
# capitalize first letter
manufacturer = stringr::str_to_title(manufacturer),
# turn into lumped factors with capitalized names
manufacturer = forcats::fct_lump(manufacturer, n = 10)
) |>
# count and sort ocurrences
dplyr::count(manufacturer, sort = TRUE) |>
dplyr::mutate(
#  order factor levels by number, put "Other" to end
manufacturer = forcats::fct_rev(forcats::fct_inorder(manufacturer)),
manufacturer = forcats::fct_relevel(manufacturer, "Other", after = 0)
)
# we have reversed the ordering since {ggplot2} plots factors from bottom to top when being mapped to y
mpg_sum
# plotting the basic bar plot
ggplot(mpg_sum, aes(x = n, y = manufacturer)) +
geom_col(fill = "gray70") +
theme_minimal()
# calculate percentages creating a temp df
# option 1: using sprintf() to create percentage labels
mpg_sum <- mpg_sum |>
dplyr::mutate(
perc = paste0(sprintf("%4.1f", n / sum(n) * 100), "%")
)
mpg_sum
# option 2: using the percent() from the scales package
# mpg_sum <- mpg_sum |>
#   dplyr::mutate(
#     perc = scales::percent(n / sum(n), accuracy = .1, trim = FALSE)
#   )
# mpg_sum
# adding the percentage label
ggplot(mpg_sum, aes(x = n, y = manufacturer)) +
geom_col(fill = "gray70") +
geom_text(aes(label = perc)) +
theme_minimal()
# adding some description to one of the bars
mpg_sum <- mpg_sum |>
dplyr::mutate(
perc = paste0(sprintf("%4.1f", n / sum(n) * 100), "%"),
perc = if_else(row_number() == 1, paste(perc, "of all car models"), perc)
)
ggplot(mpg_sum, aes(x = n, y = manufacturer)) +
geom_col(fill = "gray70") +
geom_text(aes(label = perc)) +
theme_minimal()
# example of creating and placing labels on the fly
# prepare non-aggregated data set with lumped and ordered factors
# mpg_fct <- mpg %>%
#   dplyr::filter(year == 2008) %>%
#   dplyr::mutate(
#     # add count to calculate percentages later
#     total = dplyr::n(),
#     # turn into lumped factors with capitalized names
#     manufacturer = stringr::str_to_title(manufacturer),
#     manufacturer = forcats::fct_lump(manufacturer, n = 10),
#     # order factor levels by number, put "Other" to end
#     manufacturer = forcats::fct_rev(forcats::fct_infreq(manufacturer)),
#     manufacturer = forcats::fct_relevel(manufacturer, "Other", after = 0)
#   )
# mpg_fct
#
# ggplot(mpg_fct, aes(x = manufacturer)) +
#   geom_bar(fill = "gray70") +
#   # add count labels
#   geom_text(
#     stat = "count",
#     aes(label = ..count..)
#   ) +
#   # rotate plot
#   coord_flip() +
#   theme_minimal()
# locating labels inside the bars
ggplot(mpg_sum, aes(x = n, y = manufacturer)) +
geom_col(fill = "gray70") +
geom_text(aes(label = perc),
hjust = 1,
nudge_x = -.5
) +
theme_minimal()
# In case you want to put the next to the bars, you often need to adjust the plot margin and/or the limits to avoid that the labels are cut off. The drawback of using limits is that you have to define them manually.You can make sure that labels are not truncated by the panel by adding clip = "off" to any coordinate system.
# adding colors to the bars using different hues
# option 1: create color palette based on input data
pal <- c(
"gray85",
# use the length of the manufacturer column for all non-highlighted bars and subtract the number of bars we want to highlight
rep("gray70", length(mpg_sum$manufacturer) - 4),
"coral2", "mediumpurple1", "goldenrod1"
)
ggplot(mpg_sum, aes(x = n, y = manufacturer, fill = manufacturer)) +
geom_col() +
geom_text(aes(label = perc),
hjust = 1,
nudge_x = -.5
) +
# add custom colors
scale_fill_manual(values = pal, guide = "none") +
theme_minimal()
# option 2: add the color to the data set and map the fill to that column and use scale_fill_identity()
# this option will work also if the data were updated!
mpg_sum <- mpg_sum  |>
mutate(
color = case_when(
row_number() == 1 ~ "goldenrod1",
row_number() == 2 ~ "mediumpurple1",
row_number() == 3 ~ "coral2",
manufacturer == "Other" ~ "gray85",
# all others should be gray
TRUE ~ "gray70"
)
)
ggplot(mpg_sum, aes(x = n, y = manufacturer, fill = color)) +
geom_col() +
geom_text(
aes(label = perc),
hjust = 1, nudge_x = -.5
) +
# add custom colors
scale_fill_identity(guide = "none") +
theme_minimal()
# some polishing
ggplot(mpg_sum, aes(x = n, y = manufacturer, fill = color)) +
geom_col() +
geom_text(
aes(label = perc),
hjust = 1, nudge_x = -.5,
size = 3.5, fontface = "bold", family = "Fira Sans"
) +
scale_x_continuous(expand = c(.01, .01)) +
# add custom colors
scale_fill_identity(guide = "none") +
theme_void() +
theme(
axis.text.y = element_text(size = 14, hjust = 1, family = "Fira Sans"),
plot.margin = margin(rep(15, 4))
)
# adding label boxes for accessibility
ggplot(mpg_sum, aes(x = n, y = manufacturer, fill = color)) +
geom_col() +
geom_label(
aes(label = perc),
hjust = 1, nudge_x = -.5,
size = 3.5, fontface = "bold", family = "Fira Sans",
fill = "white", label.size = 0
) +
scale_x_continuous(expand = c(.01, .01)) +
# add custom colors
scale_fill_identity(guide = "none") +
theme_void() +
theme(
axis.text.y = element_text(size = 14, hjust = 1, family = "Fira Sans"),
plot.margin = margin(rep(15, 4))
)
# with a different label placement
mpg_sum2 <- mpg_sum |>
mutate(
# set justification based on data
# so that only the first label is placed inside
place = if_else(row_number() == 1, 1, 0),
# add some spacing to labels since we cant use nudge_x anymore
perc = paste(" ", perc, " ")
)
mpg_sum2
ggplot(mpg_sum2, aes(x = n, y = manufacturer, fill = color)) +
geom_col() +
geom_text(
aes(label = perc, hjust = place),
size = 4, fontface = "bold", family = "Fira Sans"
) +
scale_x_continuous(expand = c(.01, .01)) +
scale_fill_identity(guide = "none") +
theme_void() +
theme(
axis.text.y = element_text(size = 14, hjust = 1, family = "Fira Sans"),
plot.margin = margin(rep(15, 4))
)
# lintr::lint("main.Rmd", linters =
#               lintr::with_defaults(
#                 commented_code_linter = NULL,
#                 trailing_whitespace_linter = NULL
#                 )
#             )
# # if you have additional scripts and want them to be linted too, add them here
# lintr::lint("scripts/my_script.R")
install.packages("ds4psy ")
install.packages("ds4psy")
## By defult, show code for all chunks in the knitted document,
## as well as the output. To override for a particular chunk
## use echo = FALSE in its options.
knitr::opts_chunk$set(
echo=TRUE, message=FALSE, warning=FALSE
)
# CONFIG
user_name <- "fernandomillanvillalobos" # your Git username (only needed if
# you want to deploy to GH pages)
project_name <- "datavizR" # adapt!
package_date <- "2021-06-01" # date of the CRAN snapshot that
# the checkpoint package uses
r_version <- "4.1.0" # R-Version to use
options(Ncpus = 4) # use 4 cores for parallelized installation of packages
if (r_version != paste0(version$major, ".", version$minor)) {
stop("ERROR: specified R version does not match currently used.")
}
detach_all_packages <- function() {
basic_packages_blank <-  c("stats",
"graphics",
"grDevices",
"utils",
"datasets",
"methods",
"base")
basic_packages <- paste("package:", basic_packages_blank, sep = "")
package_list <- search()[
ifelse(unlist(gregexpr("package:", search())) == 1, TRUE, FALSE)]
package_list <- setdiff(package_list, basic_packages)
if (length(package_list) > 0)  for (package in package_list) {
detach(package, character.only = TRUE, unload = TRUE)
print(paste("package ", package, " detached", sep = ""))
}
}
detach_all_packages()
# this allows multiple persons to use the same RMarkdown
# without adjusting the working directory by themselves all the time
source("scripts/csf.R")
path_to_wd <- csf() # if this - for some reason - does not work,
# replace with a hardcoded path, like so: "~/projects/rddj-template/analysis/"
if (is.null(path_to_wd) | !dir.exists(path_to_wd)) {
print("WARNING: No working directory specified for current user")
} else {
setwd(path_to_wd)
}
# suppress scientific notation
options(scipen = 999)
# suppress summarise info
options(dplyr.summarise.inform = FALSE)
# unload global rstudioapi and knitr again to avoid conflicts with checkpoint
# this is only necessary if executed within RStudio
# outside of RStudio, namely in the knit.sh script, this causes RMarkdown
# rendering to fail, thus should not be executed there
if (Sys.getenv("RSTUDIO") == "1") {
detach_all_packages()
}
# from https://mran.revolutionanalytics.com/web/packages/\
# checkpoint/vignettes/using-checkpoint-with-knitr.html
# if you don't need a package, remove it from here (commenting not sufficient)
# tidyverse: see https://blog.rstudio.org/2016/09/15/tidyverse-1-0-0/
cat("
library(rstudioapi)
library(tidyverse, warn.conflicts = FALSE) # ggplot2, dplyr, tidyr, readr, purrr, tibble, magrittr, readxl
library(scales) # scales for ggplot2
library(lintr) # code linting
library(rmarkdown)
library(cowplot) # theme
library(extrafont)
library(ggrepel) # text labels
library(gapminder) #data sets
library(socviz) # book Data Visualization: A Practical...
library(RColorBrewer)
library(ggforce)
library(dichromat) # palettes for color-blind
library(ggridges) # density ridges plots
library(viridis) # colors
library(palmerpenguins)
library(lubridate)
library(ggforce)
library(ggthemes) # set of themes
library(nycflights13) # ds example
library(broom) # cleans model output
library(glue) # for easy text formatting
library(ggiraph) # for interaction
library(hexbin)
library(patchwork)
library(distributional) # for dist_normal()
library(psych)
library(introdataviz)
library(ggalluvial)
library(ggdist)
library(ds4psy) # book Data Science for Psychologists
library(gganimate)",
file = "manifest.R")
# if checkpoint is not yet installed, install it (for people using this
# system for the first time)
if (!require(checkpoint)) {
if (!require(devtools)) {
install.packages("devtools", repos = "http://cran.us.r-project.org")
require(devtools)
}
devtools::install_github("RevolutionAnalytics/checkpoint",
ref = "v0.3.2", # could be adapted later,
# as of now (beginning of July 2017
# this is the current release on CRAN)
repos = "http://cran.us.r-project.org")
require(checkpoint)
}
# nolint start
if (!dir.exists("~/.checkpoint")) {
dir.create("~/.checkpoint")
}
# nolint end
# install packages for the specified CRAN snapshot date
checkpoint(snapshot_date = package_date,
project = path_to_wd,
verbose = T,
scanForPackages = T,
use.knitr = F,
R.version = r_version)
rm(package_date)
source("manifest.R")
unlink("manifest.R")
sessionInfo()
# if you want to outsource logic to other script files, see README for
# further information
# Load all visualizations functions as separate scripts
knitr::read_chunk("scripts/dviz.supp.R")
source("scripts/dviz.supp.R")
knitr::read_chunk("scripts/themes.R")
source("scripts/themes.R")
knitr::read_chunk("scripts/plot_grid.R")
source("scripts/plot_grid.R")
knitr::read_chunk("scripts/align_legend.R")
source("scripts/align_legend.R")
knitr::read_chunk("scripts/label_log10.R")
source("scripts/label_log10.R")
knitr::read_chunk("scripts/outliers.R")
source("scripts/outliers.R")
p <- ggplot(data = gapminder,
mapping = aes(x = year,
y = gdpPercap))
p + geom_line(aes(group=country))
p <- ggplot(data = gapminder,
mapping = aes(x = year,
y = gdpPercap))
p + geom_line(aes(group = country)) + facet_wrap(~ continent)
p <- ggplot(data = gapminder, mapping = aes(x = year, y = gdpPercap))
p + geom_line(color="gray70", aes(group = country)) +
geom_smooth(size = 1.1, method = "loess", se = FALSE) +
scale_y_log10(labels=scales::dollar) +
facet_wrap(~ continent, ncol = 5) +
labs(x = "Year",
y = "GDP per capita",
title = "GDP per capita on Five Continents")
p <- ggplot(data = gss_sm,
mapping = aes(x = age, y = childs))
p + geom_point(alpha = 0.2) +
geom_smooth() +
facet_grid(sex ~ race)
p <- ggplot(data = gss_sm, mapping = aes(x = bigregion))
p + geom_bar() # geom_bar called the default stat_ function associated with it, stat_count().
# We no longer have a count on the y-axis, but the proportions of the bars all have a value of 1, so all the bars are the same height. We want them to sum to 1, so that we get the number of observations per continent as a proportion of the total number of observations. This is a grouping issue again. In a sense, it’s the reverse of the earlier grouping problem we faced when we needed to tell ggplot that our yearly data was grouped by country.
p <- ggplot(data = gss_sm,
mapping = aes(x = bigregion))
p + geom_bar(mapping = aes(y = ..prop..))
# In this case, we need to tell ggplot to ignore the x-categories when calculating denominator of the proportion, and use the total number observations instead. To do so we specify group = 1 inside the aes() call. The value of 1 is just a kind of “dummy group” that tells ggplot to use the whole dataset when establishing the denominator for its prop calculations.
p <- ggplot(data = gss_sm,
mapping = aes(x = bigregion))
p + geom_bar(mapping = aes(y = ..prop.., group = 1)) # 1 is a dummy group
# Another example
p <- ggplot(data = gss_sm,
mapping = aes(x = religion, fill = religion))
p + geom_bar() + guides(fill = FALSE) #  If we set guides(fill = FALSE), the legend is removed
p <- ggplot(data = gss_sm,
mapping = aes(x = bigregion, fill = religion))
p + geom_bar() # The default output of geom_bar() is a stacked bar chart
# An alternative choice is to set the position argument to "fill".
p <- ggplot(data = gss_sm,
mapping = aes(x = bigregion, fill = religion))
p + geom_bar(position = "fill") # the bars are all the same height
# When we just wanted the overall proportions for one variable, we mapped group = 1 to tell ggplot to calculate the proportions with respect to the overall N.
p <- ggplot(data = gss_sm,
mapping = aes(x = bigregion, fill = religion))
p + geom_bar(position = "dodge",
mapping = aes(y = ..prop.., group = religion))
# We can ask ggplot to give us a proportional bar chart of religious affiliation, and then facet that by region
p <- ggplot(data = gss_sm,
mapping = aes(x = religion))
p + geom_bar(position = "dodge",
mapping = aes(y = ..prop.., group = bigregion)) +
facet_wrap(~ bigregion, ncol = 1)
# By default, the geom_histogram() function will choose a bin size for us based on a rule of thumb.
p <- ggplot(data = midwest,
mapping = aes(x = area))
p + geom_histogram()
# selecting another bin size
p <- ggplot(data = midwest,
mapping = aes(x = area))
p + geom_histogram(bins = 10)
oh_wi <- c("OH", "WI")
# subset the data
p <- ggplot(data = subset(midwest, subset = state %in% oh_wi), # %in% operator is a convenient way to filter on more than one termin a variable
mapping = aes(x = percollege, fill = state))
p + geom_histogram(alpha = 0.4, bins = 20)
# When working with a continuous variable, an alternative to binning the data and making a histogram is to calculate a kernel density estimate of the underlying distribution.
p <- ggplot(data = midwest,
mapping = aes(x = area, fill = state, color = state))
p + geom_density(alpha = 0.3)
# For geom_density(), the stat_density() function can return its default ..density.. statistic, or ..scaled.., which will give a proportional density estimate. It can also return a statistic called ..count.., which is the density times the number of points. This can be used in stacked density plots.
p <- ggplot(data = subset(midwest, subset = state %in% oh_wi),
mapping = aes(x = area, fill = state, color = state))
p + geom_density(alpha = 0.3, mapping = (aes(y = ..scaled..)))
p <- ggplot(data = titanic,
mapping = aes(x = fate, y = percent, fill = sex))
p + geom_bar(position = "dodge", stat = "identity") + theme(legend.position = "top")
unikn::Bordeaux
unikn::seecol(pal_ds4psy)
unikn::seecol(pal_bordeaux)
unikn::seecol(Seeblau)
unikn::seecol(c(Seeblau, deepskyblue)
unikn::seecol(c(Seeblau, deepskyblue))
unikn::seecol(Seeblau, deepskyblue)
unikn::seecol(deepskyblue)
unikn::seecol(c("Seeblue", "deepskyblue"))
unikn::seecol("Seeblue", "deepskyblue")
unikn::seecol("Seeblue")
unikn::seecol(Seeblue)
unikn::seecol(Seeblaue)
unikn::seecol(Seeblau)
unikn::seecol(Seeblau, "deepskyblue")
unikn::seecol(c(Seeblau, "deepskyblue"))
